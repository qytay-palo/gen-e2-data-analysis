# User Story 2: Mortality Data Cleaning and Standardization

**As a** public health analyst,  
**I want** to clean, standardize, and validate mortality datasets for all major diseases,  
**so that** I can perform reliable disease burden comparisons and trend analysis with high-quality data.

## 1. ðŸŽ¯ Acceptance Criteria

- All mortality datasets cleaned with documented transformations
- Disease names standardized across all tables (consistent naming convention)
- Temporal data validated: year fields converted to appropriate data types
- Missing values handled using domain-appropriate methods (interpolation, exclusion)
- Demographic categories standardized (age groups, gender labels)
- Data validation checks passed:
  - No negative mortality rates
  - Logical age group sequences
  - Consistent time series (no illogical spikes)
- Cleaned datasets saved to `data/3_interim/mortality_cleaned/`
- Data cleaning report saved to `logs/etl/data_cleaning_mortality_{timestamp}.md`

## 2. ðŸ”’ Technical Constraints

- **Data Processing**: Use Polars for efficient transformations
- **Data Quality**: Maintain data lineage for all transformations
- **Validation**: Apply domain-specific validation rules for mortality data
- **Documentation**: Log all cleaning operations with before/after metrics

## 3. ðŸ“š Domain Knowledge References

- [Disease Burden Analysis: Data Quality Considerations](../../../domain_knowledge/disease-burden-mortality-analysis.md#data-quality-considerations) - Domain-specific quality issues and mitigation strategies
- [ASMR Validation Guidelines](../../../domain_knowledge/disease-burden-mortality-analysis.md#age-standardized-mortality-rate-asmr) - Ensure proper standardization

## 4. ðŸ“¦ Dependencies

**External Packages:**
- **polars**: Data transformation and validation
- **numpy**: Statistical calculations for outlier detection
- **loguru**: Transformation logging

**Internal Dependencies:**
- Story 1 output: Raw mortality data in `data/1_raw/mortality/`
- `src/utils/logger.py`: Logging utilities
- `src/data_processing/data_profiler.py`: Validation functions

## 5. âœ… Implementation Tasks

### Disease Name Standardization
- â¬œ Create disease name mapping dictionary (raw â†’ standardized names)
- â¬œ Standardize disease names: "Malignant Neoplasms" â†’ "Cancer"
- â¬œ Apply mapping across all mortality tables consistently
- â¬œ Validate no duplicate disease entries after standardization
- â¬œ Document disease classification scheme

### Temporal Data Validation
- â¬œ Convert year columns to integer data type
- â¬œ Validate year range: 1990-2019 (expected mortality data period)
- â¬œ Check for illogical year sequences (e.g., 1899 instead of 1999)
- â¬œ Sort data by disease and year chronologically
- â¬œ Identify and document any temporal gaps

### Missing Value Handling
- â¬œ Identify patterns in missing ASMR values (specific years? diseases?)
- â¬œ For isolated missing years (1-2 years): Apply linear interpolation
- â¬œ For systematic gaps (>3 consecutive years): Document and exclude
- â¬œ Document missing value decisions in cleaning log
- â¬œ Flag interpolated values for transparency in analysis

### Outlier Detection and Validation
- â¬œ Calculate z-scores for ASMR values by disease
- â¬œ Flag values >3 standard deviations from disease-specific mean
- â¬œ Manually review flagged outliers for data entry errors
- â¬œ Validate outliers against external sources (WHO mortality database)
- â¬œ Document verified outliers as legitimate vs. corrected errors

### Demographic Standardization
- â¬œ Standardize age group labels (e.g., "0-14", "15-44", "45-64", "65+")
- â¬œ Standardize gender labels: "Male", "Female" (consistent capitalization)
- â¬œ Validate age group sequences are logical and complete
- â¬œ Check for demographic category consistency across diseases
- â¬œ Document demographic breakdown schema

### Data Type Optimization
- â¬œ Convert year to Int16 (range 1990-2100 sufficient)
- â¬œ Convert ASMR values to Float32 (sufficient precision)
- â¬œ Convert disease names to Categorical (memory efficient)
- â¬œ Convert demographic categories to Categorical
- â¬œ Measure memory reduction from type optimization

### Data Validation Checks
- â¬œ Validate no negative ASMR values exist
- â¬œ Check ASMR values are within reasonable range (0-500 per 100k)
- â¬œ Validate time series monotonicity (no abrupt unexplained changes >50%)
- â¬œ Check demographic totals match overall mortality where applicable
- â¬œ Validate all diseases have minimum 20 years of clean data
- â¬œ Run completeness checks: >95% non-null for critical fields

### Output and Documentation
- â¬œ Save cleaned datasets to `data/3_interim/mortality_cleaned/`
- â¬œ Generate data cleaning report with before/after metrics
- â¬œ Document all transformations applied (disease mapping, interpolations)
- â¬œ Create data lineage log showing transformation pipeline
- â¬œ Save cleaning validation results to `logs/etl/`
- â¬œ Update data dictionary with cleaned schema

## 6. Notes

**Interpolation Guidelines:**
- Use linear interpolation ONLY for isolated missing years (1-2 consecutive)
- For demographic breakdowns, interpolate within demographic groups
- Flag all interpolated values in a separate column for transparency

**Outlier Review Process:**
- Outliers flagged automatically but require manual review
- Cross-reference with WHO Global Health Observatory for validation
- Document justification for keeping or correcting outliers

**Quality Metrics to Track:**
- Percentage of values imputed/interpolated
- Number of outliers detected and handled
- Memory reduction from data type optimization
- Completeness improvement from cleaning

**Related Stories:**
- Depends on: Story 1 (Data Extraction and Profiling)
- Enables: Story 3 (Exploratory Disease Burden Analysis)
